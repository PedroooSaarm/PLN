{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# **Modelos del lenguaje basados en redes neuronales artificiales**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modelos seq2seq 2**\n",
    "\n",
    "Cuando la entrada de cada paso del decodificador proviene de la salida del paso anterior, estamos hablando de un modelo Seq2Seq con realimentación (feedback). Esto es especialmente común en tareas como la generación de texto.\n",
    "\n",
    "1. **Inicio de la Secuencia**: Se inicia la generación con un token especial, como `<SOS>` (Start of Sequence).\n",
    "\n",
    "2. **Generación Paso a Paso**:\n",
    "   - En el primer paso, el decodificador recibe el `<SOS>` y el estado del codificador como entrada.\n",
    "   - El decodificador procesa esta entrada y genera una salida para este paso.\n",
    "   - La salida generada se convierte en la entrada para el siguiente paso, junto con el estado actualizado del decodificador.\n",
    "   - Este proceso se repite hasta que se genera un token especial de fin de secuencia (`<EOS>`, End of Sequence) o hasta alcanzar un límite máximo de longitud.\n",
    "\n",
    "3. **Ventajas y Desventajas**:\n",
    "   - **Ventajas**: Esta forma de generar secuencias puede ayudar a mantener la coherencia en las secuencias generadas, ya que cada nueva palabra o token tiene en cuenta lo que ya se ha generado.\n",
    "   - **Desventajas**: Puede ser más lento, ya que cada paso depende del anterior, y errores en un paso pueden propagarse y afectar los pasos siguientes.\n",
    "\n",
    "\n",
    "Imagina que tienes un modelo seq2seq entrenado para traducir inglés a español. Para la frase \"How are you?\", el proceso sería algo así:\n",
    "\n",
    "1. El codificador procesa \"How are you?\" y genera un vector latente.\n",
    "2. El decodificador recibe el vector latente y el token `<SOS>`.\n",
    "3. El decodificador genera \"¿Cómo\", actualiza su estado y toma \"¿Cómo\" como entrada para el siguiente paso.\n",
    "4. El decodificador genera \"estás\", actualiza su estado y toma \"estás\" como entrada para el siguiente paso.\n",
    "5. Y así sucesivamente, hasta generar `<EOS>` para indicar el final de la secuencia.\n",
    "\n",
    "Este modelo de generación permite que el decodificador tenga en cuenta no solo el contexto proporcionado por el codificador, sino también la estructura de la secuencia que está generando, paso a paso.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/seq2seq_feedback.svg\" width=\"70%\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### **Teacher Forcing**\n",
    "\n",
    "**Teacher forcing** es una técnica utilizada en el entrenamiento de modelos seq2seq en la que, en un porcentaje de las veces, se utiliza la salida real (etiqueta) de un paso de tiempo como entrada para el siguiente paso, en lugar de la salida predicha por el modelo. Esta técnica puede ayudar a acelerar la convergencia y mejorar el rendimiento del modelo, especialmente en las etapas iniciales del entrenamiento.\n",
    "\n",
    "#### **¿Cómo funciona?**\n",
    "\n",
    "1. **Durante el Entrenamiento**: En cada paso de tiempo y con una cierta probabilidad de que suceda, en lugar de pasar la predicción del modelo del paso anterior como entrada al siguiente paso, se pasa la palabra real de la secuencia objetivo. Esto proporciona al modelo información directa y clara sobre cómo debería haber respondido en el paso anterior, independientemente de si la predicción fue correcta o no.\n",
    "\n",
    "2. **Durante la Evaluación/Predicción**: El modelo debe generar secuencias por sí mismo, utilizando sus propias predicciones del paso anterior para el siguiente paso. Durante esta fase, no se utiliza \"teacher forcing\".\n",
    "\n",
    "#### **Ventajas de Teacher Forcing:**\n",
    "\n",
    "1. **Aprendizaje más Rápido**: Al proporcionar al modelo la respuesta correcta en cada paso, se reduce la propagación de errores a través de la secuencia, lo que puede llevar a un aprendizaje más rápido.\n",
    "\n",
    "2. **Mejor Rendimiento**: Puede resultar en un mejor rendimiento del modelo, especialmente en las primeras etapas del entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementación traductor inglés a español**\n",
    "\n",
    "#### **Dataset Europarl**\n",
    "\n",
    "El conjunto de datos Europarl contiene las transcripciones de los procedimientos del Parlamento Europeo, proporcionando una valiosa fuente de textos paralelos en 21 idiomas europeos. Las oraciones están alineadas entre los idiomas, lo que lo hace especialmente útil para tareas de traducción automática. \n",
    "\n",
    "Descargamos el dataset Europarl para español-inglés. Una vez descargado tendremos dos ficheros de texto, uno para cada idioma con las frases alineadas. El código siguiente muestra las primeras frases de cada fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchtext\n",
    "import torch\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translation(Dataset):\n",
    "    def __init__(self, source_file, target_file):\n",
    "        self.ingles = []\n",
    "        self.espanol = []\n",
    "        self.tokenizer_es = get_tokenizer(\"spacy\", language=\"es_core_news_md\")\n",
    "        self.tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_md\")\n",
    "        self.vocab_es = torchtext.vocab.FastText(language='es', unk_init=torch.Tensor.normal_)  # <-- Mirar esto para ver si añadir el token <unk> al vocabulario\n",
    "        self.vocab_en = torchtext.vocab.FastText(language='en', unk_init=torch.Tensor.normal_)\n",
    "\n",
    "        self.vocab_en = self.add_sos_eos_unk_pad(self.vocab_en)\n",
    "        self.vocab_es = self.add_sos_eos_unk_pad(self.vocab_es)\n",
    "\n",
    "        self.archivo_ingles = source_file\n",
    "        self.archivo_espanol = target_file\n",
    "\n",
    "        # Leer el conjunto de datos\n",
    "        for ingles, espanol in self.read_translation():\n",
    "            self.ingles.append(ingles)\n",
    "            self.espanol.append(espanol)\n",
    "\n",
    "\n",
    "    def add_sos_eos_unk_pad(self, vocabulary):\n",
    "        words = vocabulary.itos\n",
    "        vocab = vocabulary.stoi\n",
    "        embedding_matrix = vocabulary.vectors\n",
    "\n",
    "        # Tokens especiales\n",
    "        sos_token = '<sos>'\n",
    "        eos_token = '<eos>'\n",
    "        pad_token = '<pad>'\n",
    "        unk_token = '<unk>'\n",
    "\n",
    "        # Inicializamos los vectores para los tokens especiales, por ejemplo, con ceros\n",
    "        sos_vector = torch.full((1, embedding_matrix.shape[1]), 1.)\n",
    "        eos_vector = torch.full((1, embedding_matrix.shape[1]), 2.)\n",
    "        pad_vector = torch.zeros((1, embedding_matrix.shape[1]))\n",
    "        unk_vector = torch.full((1, embedding_matrix.shape[1]), 3.)\n",
    "\n",
    "        # Añade los vectores al final de la matriz de embeddings\n",
    "        embedding_matrix = torch.cat((embedding_matrix, sos_vector, eos_vector, unk_vector, pad_vector), 0)\n",
    "\n",
    "        # Añade los tokens especiales al vocabulario\n",
    "        vocab[sos_token] = len(vocab)\n",
    "        vocab[eos_token] = len(vocab)\n",
    "        vocab[pad_token] = len(vocab)\n",
    "        vocab[unk_token] = len(vocab)\n",
    "\n",
    "        words.append(sos_token)\n",
    "        words.append(eos_token)\n",
    "        words.append(pad_token)\n",
    "        words.append(unk_token)\n",
    "\n",
    "        vocabulary.itos = words\n",
    "        vocabulary.stoi = vocab\n",
    "        vocabulary.vectors = embedding_matrix\n",
    "\n",
    "        default_stoi = defaultdict(lambda : len(vocabulary)-1, vocabulary.stoi)\n",
    "        vocabulary.stoi = default_stoi\n",
    "    \n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "    def read_translation(self):\n",
    "        with open(self.archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(self.archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "            for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "                yield oracion_ingles.strip().lower(), oracion_espanol.strip().lower()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ingles[idx], self.espanol[idx]\n",
    "        tokens_ingles = self.tokenizer_en(item[0])\n",
    "        tokens_espanol = self.tokenizer_es(item[1])\n",
    "\n",
    "        tokens_ingles = tokens_ingles + ['<eos>']\n",
    "        tokens_espanol = ['<sos>'] + tokens_espanol + ['<eos>']\n",
    "\n",
    "        if not tokens_ingles or not tokens_espanol:\n",
    "            return torch.zeros(1, 300), torch.zeros(1, 300)\n",
    "            # raise RuntimeError(\"Una de las muestras está vacía.\")\n",
    "    \n",
    "        tensor_ingles = self.vocab_en.get_vecs_by_tokens(tokens_ingles)\n",
    "        tensor_espanol = self.vocab_es.get_vecs_by_tokens(tokens_espanol)\n",
    "\n",
    "        indices_ingles = [self.vocab_en.stoi[token] for token in tokens_ingles] + [self.vocab_en.stoi['<pad>']]\n",
    "        indices_espanol = [self.vocab_es.stoi[token] for token in tokens_espanol] + [self.vocab_es.stoi['<pad>']]\n",
    "\n",
    "        return tensor_ingles, tensor_espanol, indices_ingles, indices_espanol\n",
    "        \n",
    "            \n",
    "        \n",
    "def collate_fn(batch):\n",
    "    ingles_batch, espanol_batch, ingles_seqs, espanol_seqs = zip(*batch)\n",
    "    ingles_batch = pad_sequence(ingles_batch, batch_first=True, padding_value=0)\n",
    "    espanol_batch = pad_sequence(espanol_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Calcular la longitud máxima de la lista de listas de índices\n",
    "    pad = espanol_seqs[0][-1]  # token <pad>\n",
    "    max_len = max([len(l) for l in espanol_seqs])\n",
    "    for seq in espanol_seqs:\n",
    "        seq += [pad]*(max_len-len(seq))\n",
    "        \n",
    "    return ingles_batch, espanol_batch, ingles_seqs, espanol_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archivo_ingles = 'mock.en'\n",
    "archivo_espanol = 'mock.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = 985671\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__() \n",
    "        \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"\\nEncoder\")\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        print(\"Encoder output shape: \", output.shape)\n",
    "    \n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True)  # TO DO: Añadir dropout \n",
    "        self.fc_out = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        print(\"\\nDecoder\")\n",
    "        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
    "        output = self.fc_out(output)\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query, value):\n",
    "        print(\"\\nAttention\")\n",
    "        attn_energy = torch.sum(query*value, dim=2)\n",
    "\n",
    "        return F.softmax(attn_energy, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.es_embeddings = nn.Embedding.from_pretrained(translation.vocab_es.vectors, freeze=True)\n",
    "        self.M = torch.cat((self.es_embeddings.weight, torch.zeros((0, self.es_embeddings.weight.shape[1]))), 0)\n",
    "        self.attention = BahdanauAttention(hidden_dim)\n",
    "        self.concat = nn.Linear(512*2, 512)\n",
    "        self.out = nn.Linear(512, 985671)\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        print(\"\\n\\nSeq2Seq\")\n",
    "        print(\"Source shape: \", source.shape) # (batch_size, seq_len, input_dim)\n",
    "        print(\"Target shape: \", target.shape) # (batch_size, seq_len, input_dim)\n",
    "        print(\"\\nVamos a entrar en el encoder\")\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(source)\n",
    "        print(\"\\nHemos salido del encoder\")\n",
    "        batch_size = target.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        outputs = []\n",
    "        contexts = []\n",
    "\n",
    "\n",
    "        x = target[:, 0, :]\n",
    "        print(\"X shape: \", x.shape)\n",
    "        for t in range(0, target_len):\n",
    "            print(\"\\nVamos a entrar en el decoder\")\n",
    "            print(\"X.unsqueeze(1):\",x.unsqueeze(1).shape, \"\\nHidden\", hidden.shape, \"\\nCell:\",cell.shape)\n",
    "            \n",
    "            output, (decoder_hidden, cell) = self.decoder(x.unsqueeze(1), hidden, cell)\n",
    "            \n",
    "            print(\"\\nHemos salido del decoder\")\n",
    "            print(\"Output shape: \", output.shape)\n",
    "            print(\"Hidden shape: \", hidden.shape)\n",
    "            print(\"Cell shape: \", cell.shape)\n",
    "            print(\"\\nVamos a entrar en la atención\")\n",
    "            \n",
    "            attention_weights = self.attention(output, encoder_outputs)\n",
    "            context = attention_weights.bmm(encoder_outputs)\n",
    "\n",
    "            print(\"\\nHemos salido de la atención\")\n",
    "\n",
    "            teacher_force = 0 < teacher_forcing_ratio\n",
    "            if teacher_force:\n",
    "                x = target[:, t, :]\n",
    "            else:\n",
    "\n",
    "                x = torch.matmul(context.squeeze(1), self.M)\n",
    "            outputs.append(output)\n",
    "            contexts.append(context)\n",
    "        print(\"Outputs shape: \", outputs)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        contexts = torch.cat(contexts, dim=1)\n",
    "        print(\"Outputs shape: \", outputs.shape)\n",
    "        print(\"Contexts shape: \", contexts.shape)    \n",
    "\n",
    "        concat_input =  torch.cat((outputs, contexts), 2)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "            # return output predict\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(input_dim, hidden_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(translation, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Seq2Seq\n",
      "Source shape:  torch.Size([7, 3, 300])\n",
      "Target shape:  torch.Size([7, 4, 300])\n",
      "\n",
      "Vamos a entrar en el encoder\n",
      "\n",
      "Encoder\n",
      "Encoder output shape:  torch.Size([7, 3, 512])\n",
      "\n",
      "Hemos salido del encoder\n",
      "X shape:  torch.Size([7, 300])\n",
      "\n",
      "Vamos a entrar en el decoder\n",
      "X.unsqueeze(1): torch.Size([7, 1, 300]) \n",
      "Hidden torch.Size([1, 7, 512]) \n",
      "Cell: torch.Size([1, 7, 512])\n",
      "\n",
      "Decoder\n",
      "\n",
      "Hemos salido del decoder\n",
      "Output shape:  torch.Size([7, 1, 512])\n",
      "Hidden shape:  torch.Size([1, 7, 512])\n",
      "Cell shape:  torch.Size([1, 7, 512])\n",
      "\n",
      "Vamos a entrar en la atención\n",
      "\n",
      "Attention\n",
      "\n",
      "Hemos salido de la atención\n",
      "\n",
      "Vamos a entrar en el decoder\n",
      "X.unsqueeze(1): torch.Size([7, 1, 300]) \n",
      "Hidden torch.Size([1, 7, 512]) \n",
      "Cell: torch.Size([1, 7, 512])\n",
      "\n",
      "Decoder\n",
      "\n",
      "Hemos salido del decoder\n",
      "Output shape:  torch.Size([7, 1, 512])\n",
      "Hidden shape:  torch.Size([1, 7, 512])\n",
      "Cell shape:  torch.Size([1, 7, 512])\n",
      "\n",
      "Vamos a entrar en la atención\n",
      "\n",
      "Attention\n",
      "\n",
      "Hemos salido de la atención\n",
      "\n",
      "Vamos a entrar en el decoder\n",
      "X.unsqueeze(1): torch.Size([7, 1, 300]) \n",
      "Hidden torch.Size([1, 7, 512]) \n",
      "Cell: torch.Size([1, 7, 512])\n",
      "\n",
      "Decoder\n",
      "\n",
      "Hemos salido del decoder\n",
      "Output shape:  torch.Size([7, 1, 512])\n",
      "Hidden shape:  torch.Size([1, 7, 512])\n",
      "Cell shape:  torch.Size([1, 7, 512])\n",
      "\n",
      "Vamos a entrar en la atención\n",
      "\n",
      "Attention\n",
      "\n",
      "Hemos salido de la atención\n",
      "\n",
      "Vamos a entrar en el decoder\n",
      "X.unsqueeze(1): torch.Size([7, 1, 300]) \n",
      "Hidden torch.Size([1, 7, 512]) \n",
      "Cell: torch.Size([1, 7, 512])\n",
      "\n",
      "Decoder\n",
      "\n",
      "Hemos salido del decoder\n",
      "Output shape:  torch.Size([7, 1, 512])\n",
      "Hidden shape:  torch.Size([1, 7, 512])\n",
      "Cell shape:  torch.Size([1, 7, 512])\n",
      "\n",
      "Vamos a entrar en la atención\n",
      "\n",
      "Attention\n",
      "\n",
      "Hemos salido de la atención\n",
      "Outputs shape:  [tensor([[[ 0.0108, -0.0455, -0.0745,  ..., -0.1574,  0.0615,  0.0509]],\n",
      "\n",
      "        [[ 0.0167,  0.0044, -0.0554,  ..., -0.1515,  0.0681,  0.0620]],\n",
      "\n",
      "        [[ 0.0231,  0.0049, -0.0583,  ..., -0.1464,  0.0691,  0.0557]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0127, -0.0422, -0.0721,  ..., -0.1558,  0.0646,  0.0525]],\n",
      "\n",
      "        [[ 0.0118, -0.0441, -0.0724,  ..., -0.1535,  0.0633,  0.0492]],\n",
      "\n",
      "        [[ 0.0097, -0.0465, -0.0743,  ..., -0.1559,  0.0632,  0.0498]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[ 0.0182, -0.0755, -0.1227,  ..., -0.2231,  0.0650,  0.0355]],\n",
      "\n",
      "        [[ 0.0164, -0.0268, -0.1150,  ..., -0.2204,  0.0697,  0.0459]],\n",
      "\n",
      "        [[ 0.0194, -0.0252, -0.1143,  ..., -0.2175,  0.0704,  0.0412]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0198, -0.0732, -0.1207,  ..., -0.2224,  0.0675,  0.0368]],\n",
      "\n",
      "        [[ 0.0186, -0.0746, -0.1213,  ..., -0.2209,  0.0658,  0.0349]],\n",
      "\n",
      "        [[ 0.0185, -0.0770, -0.1227,  ..., -0.2217,  0.0660,  0.0342]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[ 0.0010, -0.0323, -0.0682,  ..., -0.1075,  0.0588,  0.0326]],\n",
      "\n",
      "        [[ 0.0021, -0.0202, -0.0701,  ..., -0.1457,  0.0661,  0.0188]],\n",
      "\n",
      "        [[ 0.0034, -0.0204, -0.0673,  ..., -0.1437,  0.0646,  0.0149]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0072, -0.0474, -0.0582,  ..., -0.1063,  0.0551,  0.0179]],\n",
      "\n",
      "        [[-0.0181, -0.0464, -0.0520,  ..., -0.1242,  0.0690,  0.0153]],\n",
      "\n",
      "        [[ 0.0212, -0.0475, -0.0417,  ..., -0.1362,  0.0622,  0.0192]]],\n",
      "       grad_fn=<ViewBackward0>), tensor([[[ 0.0546, -0.0497, -0.1714,  ..., -0.2537,  0.0699, -0.0082]],\n",
      "\n",
      "        [[-0.0053,  0.0185, -0.0303,  ..., -0.0886,  0.0450,  0.0088]],\n",
      "\n",
      "        [[-0.0084, -0.0149, -0.0296,  ..., -0.0795,  0.0592,  0.0138]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0530, -0.0552, -0.1603,  ..., -0.2562,  0.0755, -0.0036]],\n",
      "\n",
      "        [[ 0.0498, -0.0555, -0.1657,  ..., -0.2662,  0.0737, -0.0113]],\n",
      "\n",
      "        [[ 0.0640, -0.0579, -0.1564,  ..., -0.2642,  0.0797, -0.0050]]],\n",
      "       grad_fn=<ViewBackward0>)]\n",
      "Outputs shape:  torch.Size([7, 4, 512])\n",
      "Contexts shape:  torch.Size([7, 4, 512])\n",
      "\n",
      "\n",
      "Salimos de Seq2Seq\n",
      "Output shape:  torch.Size([7, 4, 985671])\n",
      "tensor([[985667,   3261, 985668, 985669, 985669],\n",
      "        [985667,    403,   6361, 985668, 985669],\n",
      "        [985667,    403,  51214, 985668, 985669],\n",
      "        [985667,    895,   2942, 985668, 985669],\n",
      "        [985667,    396, 985668, 985669, 985669],\n",
      "        [985667,   4183, 985668, 985669, 985669],\n",
      "        [985667,    217, 985668, 985669, 985669]])\n",
      "tensor([[985667,   3261, 985668, 985669, 985669],\n",
      "        [985667,    403,   6361, 985668, 985669],\n",
      "        [985667,    403,  51214, 985668, 985669],\n",
      "        [985667,    895,   2942, 985668, 985669],\n",
      "        [985667,    396, 985668, 985669, 985669],\n",
      "        [985667,   4183, 985668, 985669, 985669],\n",
      "        [985667,    217, 985668, 985669, 985669]])\n",
      "tensor([[985667,   3261, 985668, 985669, 985669],\n",
      "        [985667,    403,   6361, 985668, 985669],\n",
      "        [985667,    403,  51214, 985668, 985669],\n",
      "        [985667,    895,   2942, 985668, 985669],\n",
      "        [985667,    396, 985668, 985669, 985669],\n",
      "        [985667,   4183, 985668, 985669, 985669],\n",
      "        [985667,    217, 985668, 985669, 985669]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# loss = criterion(output, torch.tensor(tgt_indices, dtype=torch.long))\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 20\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:434\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 434\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output,hidden = model(src, tgt)\n",
    "        print(\"\\n\\nSalimos de Seq2Seq\")\n",
    "        print(\"Output shape: \", output.shape)\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            print(tgt_indices)\n",
    "            if t == 1:\n",
    "                exit()\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "        # loss = criterion(output, torch.tensor(tgt_indices, dtype=torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(dataloader):.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
