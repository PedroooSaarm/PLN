{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo actual: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verificar si CUDA está disponible\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Imprimir el dispositivo actual\n",
    "print(\"Dispositivo actual:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés: Resumption of the session\n",
      "Español: Reanudación del período de sesiones\n",
      "---\n",
      "Inglés: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Español: Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n",
      "---\n",
      "Inglés: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "Español: Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n",
      "---\n",
      "Inglés: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "Español: Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n",
      "---\n",
      "Inglés: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Español: A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n",
      "---\n",
      "Inglés: Please rise, then, for this minute' s silence.\n",
      "Español: Invito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n",
      "---\n",
      "Inglés: (The House rose and observed a minute' s silence)\n",
      "Español: (El Parlamento, de pie, guarda un minuto de silencio)\n",
      "---\n",
      "Inglés: Madam President, on a point of order.\n",
      "Español: Señora Presidenta, una cuestión de procedimiento.\n",
      "---\n",
      "Inglés: You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "Español: Sabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\n",
      "---\n",
      "Inglés: One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n",
      "Español: Una de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\n",
      "---\n",
      "Inglés: Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\n",
      "Español: ¿Sería apropiado que usted, Señora Presidenta, escribiese una carta al Presidente de Sri Lanka expresando las condolencias del Parlamento por esa y otras muertes violentas, pidiéndole que haga todo lo posible para encontrar una reconciliación pacífica ante la extremadamente difícil situación que está viviendo su país?\n",
      "---\n",
      "Inglés: Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\n",
      "Español: Sí, señor Evans, pienso que una iniciativa como la que usted acaba de sugerir sería muy adecuada.\n",
      "---\n",
      "Inglés: If the House agrees, I shall do as Mr Evans has suggested.\n",
      "Español: Si la Asamblea está de acuerdo, haré lo que el señor Evans acaba de sugerir.\n",
      "---\n",
      "Inglés: Madam President, on a point of order.\n",
      "Español: Señora Presidenta, una cuestión de procedimiento.\n",
      "---\n",
      "Inglés: I would like your advice about Rule 143 concerning inadmissibility.\n",
      "Español: Me gustaría que me asesorara sobre el Artículo 143 concerniente a la inadmisibilidad.\n",
      "---\n",
      "Inglés: My question relates to something that will come up on Thursday and which I will then raise again.\n",
      "Español: Mi pregunta se refiere a un asunto del que se hablará el jueves, día que en volveré a plantearla.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def read_translation(archivo_ingles, archivo_espanol):\n",
    "    with open(archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "        for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "            yield oracion_ingles.strip(), oracion_espanol.strip()\n",
    "\n",
    "os.chdir(r\"C:\\Users\\Pedro\\Desktop\\PLN\")\n",
    "archivo_ingles = 'europarl-v7.es-en.en'\n",
    "archivo_espanol = 'europarl-v7.es-en.es'\n",
    "\n",
    "# Leer el conjunto de datos\n",
    "for i, (ingles, espanol) in enumerate(read_translation(archivo_ingles, archivo_espanol)):\n",
    "    print('Inglés:', ingles)\n",
    "    print('Español:', espanol)\n",
    "    print('---')\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchtext\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Translation(Dataset):\n",
    "    def __init__(self, source_file, target_file):\n",
    "        self.ingles = []\n",
    "        self.espanol = []\n",
    "        self.tokenizer_es = get_tokenizer(\"spacy\", language=\"es_core_news_md\")\n",
    "        self.tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_md\")\n",
    "        self.vocab_es = torchtext.vocab.FastText(language='es', unk_init=torch.Tensor.normal_)  # <-- Mirar esto para ver si añadir el token <unk> al vocabulario\n",
    "        self.vocab_en = torchtext.vocab.FastText(language='en', unk_init=torch.Tensor.normal_)\n",
    "\n",
    "        self.vocab_en = self.add_sos_eos_unk_pad(self.vocab_en)\n",
    "        self.vocab_es = self.add_sos_eos_unk_pad(self.vocab_es)\n",
    "\n",
    "        self.archivo_ingles = source_file\n",
    "        self.archivo_espanol = target_file\n",
    "\n",
    "        # Leer el conjunto de datos\n",
    "        for ingles, espanol in self.read_translation():\n",
    "            self.ingles.append(ingles)\n",
    "            self.espanol.append(espanol)\n",
    "\n",
    "\n",
    "    def add_sos_eos_unk_pad(self, vocabulary):\n",
    "        words = vocabulary.itos\n",
    "        vocab = vocabulary.stoi\n",
    "        embedding_matrix = vocabulary.vectors\n",
    "\n",
    "        # Tokens especiales\n",
    "        sos_token = '<sos>'\n",
    "        eos_token = '<eos>'\n",
    "        pad_token = '<pad>'\n",
    "        unk_token = '<unk>'\n",
    "\n",
    "        # Inicializamos los vectores para los tokens especiales, por ejemplo, con ceros\n",
    "        sos_vector = torch.full((1, embedding_matrix.shape[1]), 1.)\n",
    "        eos_vector = torch.full((1, embedding_matrix.shape[1]), 2.)\n",
    "        pad_vector = torch.zeros((1, embedding_matrix.shape[1]))\n",
    "        unk_vector = torch.full((1, embedding_matrix.shape[1]), 3.)\n",
    "\n",
    "        # Añade los vectores al final de la matriz de embeddings\n",
    "        embedding_matrix = torch.cat((embedding_matrix, sos_vector, eos_vector, unk_vector, pad_vector), 0)\n",
    "\n",
    "        # Añade los tokens especiales al vocabulario\n",
    "        vocab[sos_token] = len(vocab)\n",
    "        vocab[eos_token] = len(vocab)\n",
    "        vocab[pad_token] = len(vocab)\n",
    "        vocab[unk_token] = len(vocab)\n",
    "\n",
    "        words.append(sos_token)\n",
    "        words.append(eos_token)\n",
    "        words.append(pad_token)\n",
    "        words.append(unk_token)\n",
    "\n",
    "        vocabulary.itos = words\n",
    "        vocabulary.stoi = vocab\n",
    "        vocabulary.vectors = embedding_matrix\n",
    "\n",
    "        default_stoi = defaultdict(lambda : len(vocabulary)-1, vocabulary.stoi)\n",
    "        vocabulary.stoi = default_stoi\n",
    "    \n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "    def read_translation(self):\n",
    "        with open(self.archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(self.archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "            for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "                yield oracion_ingles.strip().lower(), oracion_espanol.strip().lower()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ingles[idx], self.espanol[idx]\n",
    "        tokens_ingles = self.tokenizer_en(item[0])\n",
    "        tokens_espanol = self.tokenizer_es(item[1])\n",
    "\n",
    "        tokens_ingles = tokens_ingles + ['<eos>']\n",
    "        tokens_espanol = ['<sos>'] + tokens_espanol + ['<eos>']\n",
    "\n",
    "        if not tokens_ingles or not tokens_espanol:\n",
    "            return torch.zeros(1, 300), torch.zeros(1, 300)\n",
    "            # raise RuntimeError(\"Una de las muestras está vacía.\")\n",
    "    \n",
    "        tensor_ingles = self.vocab_en.get_vecs_by_tokens(tokens_ingles)\n",
    "        tensor_espanol = self.vocab_es.get_vecs_by_tokens(tokens_espanol)\n",
    "\n",
    "        indices_ingles = [self.vocab_en.stoi[token] for token in tokens_ingles] + [self.vocab_en.stoi['<pad>']]\n",
    "        indices_espanol = [self.vocab_es.stoi[token] for token in tokens_espanol] + [self.vocab_es.stoi['<pad>']]\n",
    "\n",
    "        return tensor_ingles, tensor_espanol, indices_ingles, indices_espanol\n",
    "        \n",
    "            \n",
    "        \n",
    "def collate_fn(batch):\n",
    "    ingles_batch, espanol_batch, ingles_seqs, espanol_seqs = zip(*batch)\n",
    "    ingles_batch = pad_sequence(ingles_batch, batch_first=True, padding_value=0)\n",
    "    espanol_batch = pad_sequence(espanol_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Calcular la longitud máxima de la lista de listas de índices\n",
    "    pad = espanol_seqs[0][-1]  # token <pad>\n",
    "    max_len = max([len(l) for l in espanol_seqs])\n",
    "    for seq in espanol_seqs:\n",
    "        seq += [pad]*(max_len-len(seq))\n",
    "        \n",
    "    return ingles_batch, espanol_batch, ingles_seqs, espanol_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "\n",
    "archivo_ingles = 'mock.en'\n",
    "archivo_espanol = 'mock.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  # TO DO: Añadir dropout \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
    "        output = self.fc_out(output)\n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder                           \n",
    "        self.es_embeddings = torchtext.vocab.FastText(language='es')\n",
    "        self.M = self.es_embeddings.vectors\n",
    "        self.M = torch.cat((self.M, torch.zeros((4, self.M.shape[1]))), 0)\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        target_len = target.shape[1]\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        # Tensor para almacenar las salidas del decoder\n",
    "        outputs = torch.zeros(batch_size, target_len, 985671)\n",
    "        \n",
    "        # Primero, la fuente es procesada por el encoder\n",
    "        _, (hidden, cell) = self.encoder(source)\n",
    "\n",
    "        # La primera entrada al decoder es el vector <sos>\n",
    "        x = target[:, 0, :]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, (hidden, cell) = self.decoder(x.unsqueeze(1), hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if teacher_force:\n",
    "                x = target[:, t, :]\n",
    "            else:\n",
    "                x = torch.matmul(output.squeeze(1), self.M)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(translation, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.4337\n",
      "Epoch [1/30], Average Loss: 34.4237\n",
      "Epoch [2/30], Step [1/2], Loss: 40.5665\n",
      "Epoch [2/30], Average Loss: 40.1977\n",
      "Epoch [3/30], Step [1/2], Loss: 37.6587\n",
      "Epoch [3/30], Average Loss: 30.5162\n",
      "Epoch [4/30], Step [1/2], Loss: 33.2907\n",
      "Epoch [4/30], Average Loss: 32.4507\n",
      "Epoch [5/30], Step [1/2], Loss: 29.0172\n",
      "Epoch [5/30], Average Loss: 22.9772\n",
      "Epoch [6/30], Step [1/2], Loss: 25.2378\n",
      "Epoch [6/30], Average Loss: 24.5003\n",
      "Epoch [7/30], Step [1/2], Loss: 21.9994\n",
      "Epoch [7/30], Average Loss: 21.9235\n",
      "Epoch [8/30], Step [1/2], Loss: 19.6685\n",
      "Epoch [8/30], Average Loss: 14.6341\n",
      "Epoch [9/30], Step [1/2], Loss: 17.6373\n",
      "Epoch [9/30], Average Loss: 13.0504\n",
      "Epoch [10/30], Step [1/2], Loss: 16.0583\n",
      "Epoch [10/30], Average Loss: 11.5565\n",
      "Epoch [11/30], Step [1/2], Loss: 14.4137\n",
      "Epoch [11/30], Average Loss: 10.7233\n",
      "Epoch [12/30], Step [1/2], Loss: 12.5568\n",
      "Epoch [12/30], Average Loss: 12.0838\n",
      "Epoch [13/30], Step [1/2], Loss: 10.8682\n",
      "Epoch [13/30], Average Loss: 7.7104\n",
      "Epoch [14/30], Step [1/2], Loss: 9.1557\n",
      "Epoch [14/30], Average Loss: 7.1309\n",
      "Epoch [15/30], Step [1/2], Loss: 7.8998\n",
      "Epoch [15/30], Average Loss: 7.2223\n",
      "Epoch [16/30], Step [1/2], Loss: 6.7404\n",
      "Epoch [16/30], Average Loss: 5.4898\n",
      "Epoch [17/30], Step [1/2], Loss: 6.1592\n",
      "Epoch [17/30], Average Loss: 5.5285\n",
      "Epoch [18/30], Step [1/2], Loss: 5.8042\n",
      "Epoch [18/30], Average Loss: 5.2874\n",
      "Epoch [19/30], Step [1/2], Loss: 5.4479\n",
      "Epoch [19/30], Average Loss: 4.5630\n",
      "Epoch [20/30], Step [1/2], Loss: 5.2459\n",
      "Epoch [20/30], Average Loss: 4.2431\n",
      "Epoch [21/30], Step [1/2], Loss: 5.0025\n",
      "Epoch [21/30], Average Loss: 4.2254\n",
      "Epoch [22/30], Step [1/2], Loss: 4.9009\n",
      "Epoch [22/30], Average Loss: 3.8277\n",
      "Epoch [23/30], Step [1/2], Loss: 4.9410\n",
      "Epoch [23/30], Average Loss: 3.8033\n",
      "Epoch [24/30], Step [1/2], Loss: 4.5677\n",
      "Epoch [24/30], Average Loss: 5.5790\n",
      "Epoch [25/30], Step [1/2], Loss: 4.8605\n",
      "Epoch [25/30], Average Loss: 3.5149\n",
      "Epoch [26/30], Step [1/2], Loss: 4.5974\n",
      "Epoch [26/30], Average Loss: 4.7001\n",
      "Epoch [27/30], Step [1/2], Loss: 4.7068\n",
      "Epoch [27/30], Average Loss: 3.4538\n",
      "Epoch [28/30], Step [1/2], Loss: 4.5603\n",
      "Epoch [28/30], Average Loss: 4.0533\n",
      "Epoch [29/30], Step [1/2], Loss: 4.5034\n",
      "Epoch [29/30], Average Loss: 3.9143\n",
      "Epoch [30/30], Step [1/2], Loss: 4.4290\n",
      "Epoch [30/30], Average Loss: 3.8498\n"
     ]
    }
   ],
   "source": [
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "        # loss = criterion(output, torch.tensor(tgt_indices, dtype=torch.long))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomáis praedator python python alzateaceae\n"
     ]
    }
   ],
   "source": [
    "# Test the model with input sentences\n",
    "model.eval()\n",
    "\n",
    "sentence = \"dog\"\n",
    "\n",
    "# Convertir a vectores\n",
    "tokens = translation.tokenizer_en(sentence)\n",
    "tokens = tokens + ['<eos>']\n",
    "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
    "text_tensor = text_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
    "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        output, (hidden, cell) = model.decoder(input_token, hidden, cell) # teacher_forcing_ratio=0.0\n",
    "        \n",
    "    # Obtener el token con la probabilidad más alta\n",
    "    best_guess = output.argmax(2).squeeze(0)\n",
    "    outputs.append(best_guess.item())\n",
    "        \n",
    "    # Si el token es <eos>, terminar la traducción\n",
    "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
    "        break\n",
    "        \n",
    "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
    "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
    "        \n",
    "# Convertir los índices de salida a palabras\n",
    "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
    "    \n",
    "result = ' '.join(translated_sentence)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo\n",
    "torch.save(model.state_dict(), 'seq2seq.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from file\n",
    "model.load_state_dict(torch.load('seq2seq.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
