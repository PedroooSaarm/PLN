{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1\n",
    "Modifica el código anterior para adaptar el modelo LSTM al uso de embeddings preentrenados. Para ello, usa from torchtext.vocab import GloVe y elige el conjunto de embeddings GloVe que prefieras. Puedes encontrar más información en https://pytorch.org/text/stable/vocab.html#torchtext.vocab.GloVe\n",
    "\n",
    "Verifica si se produce una mejora en la precisión del modelo. ¿Qué ocurre si usas un conjunto de embeddings preentrenados de diferentes tamaños?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargamos el dataset** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext.data import to_map_style_dataset\n",
    "import numpy as np\n",
    "from torchtext.vocab import GloVe, vocab\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter, OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the dataset\n",
    "train_iter, test_iter = datasets.AG_NEWS(split=('train', 'test'))\n",
    "\n",
    "train_ds = to_map_style_dataset(train_iter)\n",
    "test_ds = to_map_style_dataset(test_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizamos el Dataset y creamos nuestro vocabulario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "my_vocab = build_vocab_from_iterator(map(lambda x: tokenizer(x[1]), train_ds), specials=['<pad>','<unk>'])\n",
    "my_vocab.set_default_index(my_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usamos DataLoader para modificar los datos para el posterior entrenamiento.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, _) in train_iter]))\n",
    "\n",
    "text_pipeline = lambda x: my_vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for sample in batch:\n",
    "        label, text = sample\n",
    "        text_list.append(torch.tensor(text_pipeline(text), dtype=torch.long))\n",
    "        label_list.append(label_pipeline(label))\n",
    "    return torch.tensor(label_list, dtype=torch.long), torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=my_vocab[\"<pad>\"])\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aquí cargamos el vocabulario de GloVe para 100, 200 y 300 dimensiones.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors_100 = GloVe(name='6B', dim=100)\n",
    "glove_vectors_200 = GloVe(name='6B', dim=200)\n",
    "glove_vectors_300 = GloVe(name='6B', dim=300)\n",
    "\n",
    "glove_vectors = [glove_vectors_100, glove_vectors_200, glove_vectors_300]\n",
    "\n",
    "unk_token = \"<unk>\"\n",
    "unk_index = 0\n",
    "\n",
    "x = 100\n",
    "\n",
    "for i in glove_vectors:\n",
    "    glove_vocab_name = f'glove_vocab_{x}'\n",
    "    glove_vocab_x = vocab(i.stoi)\n",
    "    glove_vocab_x.insert_token(unk_token, unk_index)\n",
    "    glove_vocab_x.set_default_index(unk_index)\n",
    "    globals()[glove_vocab_name] = glove_vocab_x\n",
    "    x += 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creamos un embedding para cada una de las dimensiones.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_17356\\42317065.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.stack( list( map(torch.tensor, embeddings) ) )\n"
     ]
    }
   ],
   "source": [
    "# versión 1\n",
    "def pretrain_embeddings(tokens, glove_vectors):\n",
    "    \n",
    "    embeddings = [glove_vectors[token] for token in tokens]\n",
    "\n",
    "    return torch.stack( list( map(torch.tensor, embeddings) ) )\n",
    "\n",
    "embeddings_100 = pretrain_embeddings(my_vocab.get_itos(), glove_vectors_100)\n",
    "embeddings_200 = pretrain_embeddings(my_vocab.get_itos(), glove_vectors_200)\n",
    "embeddings_300 = pretrain_embeddings(my_vocab.get_itos(), glove_vectors_300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Con los embeddings creamos tres modelos para la clasificación de texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class, pretrained_embeddings):\n",
    "        super(LSTMTextClassificationModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)  \n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        output = self.fc(last_output)\n",
    "        return output\n",
    "\n",
    "vocab_size = len(my_vocab)  \n",
    "hidden_dim = 64\n",
    "num_class = 4\n",
    "\n",
    "model_100 = LSTMTextClassificationModel(vocab_size, 100, hidden_dim, num_class, embeddings_100)\n",
    "model_200 = LSTMTextClassificationModel(vocab_size, 200, hidden_dim, num_class, embeddings_200)\n",
    "model_300 = LSTMTextClassificationModel(vocab_size, 300, hidden_dim, num_class, embeddings_300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos el modelo de 100 dimensiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model = model_100\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count, max_acc = 0, 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(idx, total_acc / total_count))\n",
    "\n",
    "            if max_acc < total_acc / total_count:\n",
    "                max_acc = total_acc / total_count\n",
    "                \n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   500 batches | accuracy    0.254\n",
      "|  1000 batches | accuracy    0.253\n",
      "|  1500 batches | accuracy    0.362\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 69.11s | valid accuracy    0.845 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.798\n",
      "|  1000 batches | accuracy    0.858\n",
      "|  1500 batches | accuracy    0.889\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 63.54s | valid accuracy    0.871 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.885\n",
      "|  1000 batches | accuracy    0.893\n",
      "|  1500 batches | accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 83.79s | valid accuracy    0.900 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.894\n",
      "|  1000 batches | accuracy    0.899\n",
      "|  1500 batches | accuracy    0.916\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 70.99s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.900\n",
      "|  1000 batches | accuracy    0.906\n",
      "|  1500 batches | accuracy    0.921\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 66.79s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.904\n",
      "|  1000 batches | accuracy    0.910\n",
      "|  1500 batches | accuracy    0.924\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 60.27s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.908\n",
      "|  1000 batches | accuracy    0.917\n",
      "|  1500 batches | accuracy    0.929\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 57.33s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.918\n",
      "|  1000 batches | accuracy    0.924\n",
      "|  1500 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 57.45s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.921\n",
      "|  1000 batches | accuracy    0.927\n",
      "|  1500 batches | accuracy    0.934\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 57.01s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.925\n",
      "|  1000 batches | accuracy    0.930\n",
      "|  1500 batches | accuracy    0.935\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 60.91s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "\n",
    "    #if accu_train > accu_val:\n",
    "    #    scheduler.step()\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos el modelo con 200 dimensiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model = model_200\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count, max_acc = 0, 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(idx, total_acc / total_count))\n",
    "\n",
    "            if max_acc < total_acc / total_count:\n",
    "                max_acc = total_acc / total_count\n",
    "                \n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   500 batches | accuracy    0.258\n",
      "|  1000 batches | accuracy    0.259\n",
      "|  1500 batches | accuracy    0.343\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 72.58s | valid accuracy    0.649 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.828\n",
      "|  1000 batches | accuracy    0.877\n",
      "|  1500 batches | accuracy    0.900\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 72.06s | valid accuracy    0.868 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.887\n",
      "|  1000 batches | accuracy    0.898\n",
      "|  1500 batches | accuracy    0.911\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 70.87s | valid accuracy    0.896 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.898\n",
      "|  1000 batches | accuracy    0.904\n",
      "|  1500 batches | accuracy    0.917\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 67.86s | valid accuracy    0.900 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.905\n",
      "|  1000 batches | accuracy    0.911\n",
      "|  1500 batches | accuracy    0.926\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 73.89s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.912\n",
      "|  1000 batches | accuracy    0.921\n",
      "|  1500 batches | accuracy    0.927\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 70.03s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.920\n",
      "|  1000 batches | accuracy    0.925\n",
      "|  1500 batches | accuracy    0.934\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 71.62s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.925\n",
      "|  1000 batches | accuracy    0.931\n",
      "|  1500 batches | accuracy    0.936\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 69.15s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.928\n",
      "|  1000 batches | accuracy    0.935\n",
      "|  1500 batches | accuracy    0.940\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 70.14s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.934\n",
      "|  1000 batches | accuracy    0.937\n",
      "|  1500 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 70.16s | valid accuracy    0.916 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "\n",
    "    #if accu_train > accu_val:\n",
    "    #    scheduler.step()\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos el modelo con 300 dimensiones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model = model_300\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count, max_acc = 0, 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| {:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(idx, total_acc / total_count))\n",
    "\n",
    "            if max_acc < total_acc / total_count:\n",
    "                max_acc = total_acc / total_count\n",
    "                \n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   500 batches | accuracy    0.256\n",
      "|  1000 batches | accuracy    0.256\n",
      "|  1500 batches | accuracy    0.282\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 83.11s | valid accuracy    0.684 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.690\n",
      "|  1000 batches | accuracy    0.786\n",
      "|  1500 batches | accuracy    0.879\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 82.27s | valid accuracy    0.878 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.883\n",
      "|  1000 batches | accuracy    0.895\n",
      "|  1500 batches | accuracy    0.910\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 85.47s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.899\n",
      "|  1000 batches | accuracy    0.905\n",
      "|  1500 batches | accuracy    0.921\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 85.77s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.908\n",
      "|  1000 batches | accuracy    0.915\n",
      "|  1500 batches | accuracy    0.925\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 83.25s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.918\n",
      "|  1000 batches | accuracy    0.922\n",
      "|  1500 batches | accuracy    0.932\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 81.16s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.922\n",
      "|  1000 batches | accuracy    0.928\n",
      "|  1500 batches | accuracy    0.938\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 82.76s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.930\n",
      "|  1000 batches | accuracy    0.934\n",
      "|  1500 batches | accuracy    0.940\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 81.09s | valid accuracy    0.917 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.934\n",
      "|  1000 batches | accuracy    0.937\n",
      "|  1500 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 83.90s | valid accuracy    0.912 \n",
      "-----------------------------------------------------------\n",
      "|   500 batches | accuracy    0.938\n",
      "|  1000 batches | accuracy    0.941\n",
      "|  1500 batches | accuracy    0.949\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 80.96s | valid accuracy    0.915 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    accu_train = train(train_dataloader)\n",
    "    accu_val = evaluate(test_dataloader)\n",
    "\n",
    "    #if accu_train > accu_val:\n",
    "    #    scheduler.step()\n",
    "    \n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusión**\n",
    "Podemos comprobar que usando GloVe se mejora la presición del modelo, y también podemos ver que cuantas más dimensiones cojas del set de embeddings de GloVe mayor será la presición del modelo, pero tampoco hay mucha diferencia entre las tres dimensiones con las que hemos entrenado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
